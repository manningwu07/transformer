#!/usr/bin/env python3
import argparse, json, os
import torch
from torch.utils.data import DataLoader
import numpy as np

from transformer import GPT2LikeLM
from params import Config
from train import IndexedBinaryDataset
from eval import evaluate

def main(args):
    device = "mps" if torch.backends.mps.is_available() else \
             "cuda" if torch.cuda.is_available() else "cpu"
    print(f"‚ö° Training LoRA adapters on {device}")

    # --- Vocab ---
    vocab = json.load(open(args.vocab))
    tok2id, id2tok = vocab["TokenToID"], vocab["IDToToken"]
    vocab_size = len(id2tok)
    pad_id = tok2id["<pad>"]

    # --- Dataset ---
    train_ds = IndexedBinaryDataset(args.train, Config.seq_len, shuffle=True, pad_id=pad_id)
    val_ds = IndexedBinaryDataset(args.val, Config.seq_len, shuffle=False, pad_id=pad_id)
    train_loader = DataLoader(train_ds, batch_size=Config.batch_size, num_workers=1)
    val_loader   = DataLoader(val_ds,   batch_size=Config.batch_size, num_workers=1)

    # --- Model ---
    model = GPT2LikeLM(
        vocab_size=vocab_size,
        d_model=Config.d_model,
        n_heads=Config.num_heads,
        n_layers=Config.n_layers,
        d_ff=Config.hidden_size,
        dropout=Config.dropout,
        max_len=Config.max_len,
        pad_id=pad_id,
        bos_id=tok2id["<bos>"],
        eos_id=tok2id["<eos>"],
        unk_id=tok2id["<unk>"],
    ).to(device)

    # --- Load base weights ---
    ckpt = torch.load(args.resumePath, map_location=device)
    state = ckpt.get("model_state", ckpt)
    model.load_state_dict(state, strict=False)
    print("‚úîÔ∏è Loaded base checkpoint.")

    # Freeze everything except LoRA params
    for name, p in model.named_parameters():
        if "A" in name or "B" in name:  # LoRA adapters
            p.requires_grad = True
        else:
            p.requires_grad = False

    # --- Optimizer ---
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)

    criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_id)

    # --- Training ---
    steps = 0
    model.train()
    while steps < args.max_steps:
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            logits, _ = model(x)
            loss = criterion(logits.view(-1, vocab_size), y.view(-1))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            steps += 1
            if steps % 100 == 0:
                print(f"Step {steps} LoRA loss={loss.item():.4f}")

            if steps % args.eval_every == 0:
                val_loss = evaluate(model, val_loader, vocab_size, pad_id, device, split="VAL", max_batches=100)
                print(f"[VAL-LORA] step {steps}, val_loss={val_loss:.4f}")

            if steps >= args.max_steps:
                break

    # Save only LoRA adapter weights
    lora_state = {k: v for k, v in model.state_dict().items() if "A" in k or "B" in k}
    os.makedirs("models", exist_ok=True)
    torch.save(lora_state, "models/lora_alpaca.pt")
    print("üíæ Saved LoRA adapters to models/lora_alpaca.pt")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train", type=str, required=True)
    parser.add_argument("--val", type=str, required=True)
    parser.add_argument("--vocab", type=str, required=True)
    parser.add_argument("--resumePath", type=str, default="models/best_model.pt")
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--max_steps", type=int, default=5000)
    parser.add_argument("--eval_every", type=int, default=500)
    args = parser.parse_args()
    main(args)