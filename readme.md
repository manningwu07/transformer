## This is the pytorch transfer of the transformer project

- Using the code here, I created a ~200M param model that becomes a math/CS version of GPT2-small; but better
    - This thing is built from scratch and I've spent way too much compute on this project (m4 pro---30 battery cycles)
- In essence, this thing is meant for testing large scale stuff
- The go-only branch was to learn the inner workings of a transfomrer, as well as how to scale to ~10-200m params

### Please check out the go-only branch for better description