## This is the pytorch transfer of the transformer project

- Using the code here, I created a ~55.2M param model that becomes a math/CS version of GPT2; but better
    - This thing is built from scratch and I've spent way too much compute on this project
- This was a fun experiment geting to architect and explore how AI and specifically LLMs work, as well as how to maximize their accuracy while minimizing memory and compute via optimial data and parameterization. 
- The go-only branch was to learn the inner workings of a transfomrer, as well as how to scale to ~10-200m params

### Please check out the go-only branch for better description